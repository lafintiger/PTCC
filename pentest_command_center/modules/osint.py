import sys
import os
import json
import time
import re
import asyncio
from typing import Dict, List, Any, Optional, Union
import requests
from bs4 import BeautifulSoup

# Add parent directory to path so we can import from config and utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import SCAN_RESULTS_DIR, SHODAN_API_KEY
from utils.command import CommandExecutor

class OSINTScanner:
    """
    Handles OSINT (Open Source Intelligence) operations for data gathering.
    """
    
    def __init__(self):
        """Initialize the OSINT scanner."""
        self.command_executor = CommandExecutor()
        self.scan_results = {}
    
    async def run_harvester(self, domain: str, source: Optional[str] = None) -> Dict[str, Any]:
        """
        Run theHarvester to gather information about a domain.
        
        Args:
            domain: Target domain
            source: Specific source to use (e.g., "all", "google", "linkedin", etc.)
            
        Returns:
            Harvester results as a dictionary
        """
        # Check if theHarvester is installed
        if not self.command_executor.is_tool_installed("theHarvester"):
            return {
                "error": "theHarvester is not installed. Please install it to use this feature.",
                "scan_info": {
                    "target": domain,
                    "tool": "theHarvester"
                }
            }
        
        # Build command
        command = f"theHarvester -d {domain} -b"
        if source:
            command += f" {source}"
        else:
            command += " all"
        
        # Execute command
        result = await self.command_executor.run_command_async(command)
        
        if result["success"]:
            # Parse the output
            parsed_results = self._parse_harvester_output(result["stdout"])
            
            # Create result object
            scan_result = {
                "scan_info": {
                    "target": domain,
                    "tool": "theHarvester",
                    "source": source or "all",
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                "results": parsed_results,
                "raw_command": result
            }
            
            # Save scan result
            scan_id = f"harvester_{domain.replace('.', '_')}_{int(time.time())}"
            self.scan_results[scan_id] = scan_result
            
            # Save to file
            self._save_scan_result(scan_id, scan_result)
            
            return scan_result
            
        else:
            return {
                "error": f"theHarvester command failed: {result['stderr']}",
                "scan_info": {
                    "target": domain,
                    "tool": "theHarvester",
                    "source": source or "all"
                },
                "raw_command": result
            }
    
    def _parse_harvester_output(self, output: str) -> Dict[str, Any]:
        """
        Parse theHarvester output into structured data.
        
        Args:
            output: Command output text
            
        Returns:
            Parsed data
        """
        result = {
            "emails": [],
            "hosts": [],
            "ips": [],
            "urls": []
        }
        
        # Extract emails
        emails_section = False
        hosts_section = False
        ips_section = False
        
        for line in output.splitlines():
            line = line.strip()
            
            # Check for section headers
            if "[*] Emails found:" in line:
                emails_section = True
                hosts_section = False
                ips_section = False
                continue
            elif "[*] Hosts found:" in line:
                emails_section = False
                hosts_section = True
                ips_section = False
                continue
            elif "[*] IPs found:" in line:
                emails_section = False
                hosts_section = False
                ips_section = True
                continue
            elif "[*]" in line and "found:" in line:  # Any other section
                emails_section = False
                hosts_section = False
                ips_section = False
                continue
            
            # Extract data based on current section
            if emails_section and "@" in line:
                result["emails"].append(line)
            elif hosts_section and line and not line.startswith("[*]"):
                result["hosts"].append(line)
            elif ips_section and re.match(r'\d+\.\d+\.\d+\.\d+', line):
                result["ips"].append(line)
            
            # Extract URLs from any section
            urls = re.findall(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', line)
            if urls:
                result["urls"].extend(urls)
        
        # Remove duplicates
        result["emails"] = list(set(result["emails"]))
        result["hosts"] = list(set(result["hosts"]))
        result["ips"] = list(set(result["ips"]))
        result["urls"] = list(set(result["urls"]))
        
        return result
    
    async def search_shodan(self, query: str) -> Dict[str, Any]:
        """
        Search Shodan for information.
        
        Args:
            query: Shodan search query (IP, hostname, etc.)
            
        Returns:
            Shodan results as a dictionary
        """
        if not SHODAN_API_KEY:
            return {
                "error": "Shodan API key is not configured. Please set the SHODAN_API_KEY in your settings.",
                "scan_info": {
                    "query": query,
                    "tool": "shodan"
                }
            }
        
        try:
            # Use requests instead of Shodan API library for simplicity
            base_url = "https://api.shodan.io"
            
            # Determine if query is an IP or a search term
            if re.match(r'\d+\.\d+\.\d+\.\d+', query):
                # IP lookup
                endpoint = f"/shodan/host/{query}"
                params = {"key": SHODAN_API_KEY}
            else:
                # Search
                endpoint = "/shodan/host/search"
                params = {"key": SHODAN_API_KEY, "query": query}
            
            response = requests.get(f"{base_url}{endpoint}", params=params)
            
            if response.status_code == 200:
                shodan_data = response.json()
                
                # Create result object
                scan_result = {
                    "scan_info": {
                        "query": query,
                        "tool": "shodan",
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    },
                    "results": shodan_data
                }
                
                # Save scan result
                scan_id = f"shodan_{query.replace('.', '_').replace(':', '_')}_{int(time.time())}"
                self.scan_results[scan_id] = scan_result
                
                # Save to file
                self._save_scan_result(scan_id, scan_result)
                
                return scan_result
                
            else:
                return {
                    "error": f"Shodan API request failed with status code {response.status_code}: {response.text}",
                    "scan_info": {
                        "query": query,
                        "tool": "shodan"
                    }
                }
                
        except Exception as e:
            return {
                "error": f"Error searching Shodan: {str(e)}",
                "scan_info": {
                    "query": query,
                    "tool": "shodan"
                }
            }
    
    async def search_subdomains(self, domain: str) -> Dict[str, Any]:
        """
        Search for subdomains of a given domain using various tools.
        
        Args:
            domain: Target domain
            
        Returns:
            Subdomain search results as a dictionary
        """
        # Check if sublist3r is installed
        has_sublist3r = self.command_executor.is_tool_installed("sublist3r")
        
        subdomains = set()
        commands_output = {}
        
        if has_sublist3r:
            # Run sublist3r
            command = f"sublist3r -d {domain} -o /tmp/subdomains_{domain}.txt"
            result = await self.command_executor.run_command_async(command)
            commands_output["sublist3r"] = result
            
            if result["success"]:
                # Parse sublist3r output
                for line in result["stdout"].splitlines():
                    if domain in line and not line.startswith("[*]") and not line.startswith("[-]"):
                        subdomains.add(line.strip())
                
                # Check if output file exists and read it
                output_file = f"/tmp/subdomains_{domain}.txt"
                if os.path.exists(output_file):
                    with open(output_file, 'r') as f:
                        for line in f:
                            subdomains.add(line.strip())
        
        # Try alternative approach with amass if available
        has_amass = self.command_executor.is_tool_installed("amass")
        
        if has_amass:
            # Run amass
            command = f"amass enum -d {domain}"
            result = await self.command_executor.run_command_async(command)
            commands_output["amass"] = result
            
            if result["success"]:
                # Parse amass output
                for line in result["stdout"].splitlines():
                    if domain in line:
                        subdomains.add(line.strip())
        
        # Use dig for NS and MX records
        dig_ns_cmd = f"dig NS {domain} +short"
        dig_mx_cmd = f"dig MX {domain} +short"
        
        dig_ns_result = await self.command_executor.run_command_async(dig_ns_cmd)
        dig_mx_result = await self.command_executor.run_command_async(dig_mx_cmd)
        
        commands_output["dig_ns"] = dig_ns_result
        commands_output["dig_mx"] = dig_mx_result
        
        # Extract nameservers
        nameservers = []
        for line in dig_ns_result["stdout"].splitlines():
            if line.strip():
                nameservers.append(line.strip())
        
        # Extract mail servers
        mail_servers = []
        for line in dig_mx_result["stdout"].splitlines():
            if line.strip():
                parts = line.split()
                if len(parts) > 1:
                    mail_servers.append(parts[1])
        
        # Create result object
        scan_result = {
            "scan_info": {
                "target": domain,
                "tool": "subdomain_scan",
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            },
            "results": {
                "subdomains": list(subdomains),
                "count": len(subdomains),
                "nameservers": nameservers,
                "mail_servers": mail_servers
            },
            "raw_commands": commands_output
        }
        
        # Save scan result
        scan_id = f"subdomains_{domain.replace('.', '_')}_{int(time.time())}"
        self.scan_results[scan_id] = scan_result
        
        # Save to file
        self._save_scan_result(scan_id, scan_result)
        
        return scan_result
    
    async def fetch_whois(self, domain: str) -> Dict[str, Any]:
        """
        Fetch WHOIS information for a domain.
        
        Args:
            domain: Target domain
            
        Returns:
            WHOIS results as a dictionary
        """
        # Check if whois command is available
        if not self.command_executor.is_tool_installed("whois"):
            return {
                "error": "whois command is not installed. Please install it to use this feature.",
                "scan_info": {
                    "target": domain,
                    "tool": "whois"
                }
            }
        
        # Run whois command
        command = f"whois {domain}"
        result = await self.command_executor.run_command_async(command)
        
        if result["success"]:
            # Parse whois output
            parsed_whois = self._parse_whois_output(result["stdout"])
            
            # Create result object
            scan_result = {
                "scan_info": {
                    "target": domain,
                    "tool": "whois",
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                "results": parsed_whois,
                "raw_command": result
            }
            
            # Save scan result
            scan_id = f"whois_{domain.replace('.', '_')}_{int(time.time())}"
            self.scan_results[scan_id] = scan_result
            
            # Save to file
            self._save_scan_result(scan_id, scan_result)
            
            return scan_result
            
        else:
            return {
                "error": f"whois command failed: {result['stderr']}",
                "scan_info": {
                    "target": domain,
                    "tool": "whois"
                },
                "raw_command": result
            }
    
    def _parse_whois_output(self, output: str) -> Dict[str, Any]:
        """
        Parse WHOIS output into structured data.
        
        Args:
            output: Command output text
            
        Returns:
            Parsed data
        """
        result = {
            "domain_name": "",
            "registrar": "",
            "created_date": "",
            "updated_date": "",
            "expiration_date": "",
            "name_servers": [],
            "registrant": {},
            "admin": {},
            "tech": {},
            "emails": [],
            "raw": output
        }
        
        # Extract key information using regex
        domain_match = re.search(r'Domain Name: (.+)', output, re.IGNORECASE)
        if domain_match:
            result["domain_name"] = domain_match.group(1).strip()
        
        registrar_match = re.search(r'Registrar: (.+)', output, re.IGNORECASE)
        if registrar_match:
            result["registrar"] = registrar_match.group(1).strip()
        
        created_match = re.search(r'Creation Date: (.+)', output, re.IGNORECASE)
        if created_match:
            result["created_date"] = created_match.group(1).strip()
        
        updated_match = re.search(r'Updated Date: (.+)', output, re.IGNORECASE)
        if updated_match:
            result["updated_date"] = updated_match.group(1).strip()
        
        expiration_match = re.search(r'Registry Expiry Date: (.+)', output, re.IGNORECASE)
        if expiration_match:
            result["expiration_date"] = expiration_match.group(1).strip()
        else:
            # Try alternative format
            expiration_match = re.search(r'Expir(y|ation) Date: (.+)', output, re.IGNORECASE)
            if expiration_match:
                result["expiration_date"] = expiration_match.group(2).strip()
        
        # Extract name servers
        ns_matches = re.findall(r'Name Server: (.+)', output, re.IGNORECASE)
        if ns_matches:
            result["name_servers"] = [ns.strip() for ns in ns_matches]
        
        # Extract emails
        email_matches = re.findall(r'[\w\.-]+@[\w\.-]+', output)
        if email_matches:
            result["emails"] = list(set(email_matches))
        
        return result
    
    async def discover_technologies(self, url: str) -> Dict[str, Any]:
        """
        Discover technologies used by a website.
        
        Args:
            url: Target URL
            
        Returns:
            Technologies detected
        """
        try:
            # Check if whatweb is installed
            has_whatweb = self.command_executor.is_tool_installed("whatweb")
            
            # Results to be collected
            technologies = set()
            commands_output = {}
            
            # Try with whatweb if available
            if has_whatweb:
                command = f"whatweb -a 3 --log-json=/tmp/whatweb_{int(time.time())}.json {url}"
                result = await self.command_executor.run_command_async(command)
                commands_output["whatweb"] = result
                
                if result["success"]:
                    # Parse whatweb output
                    for line in result["stdout"].splitlines():
                        if "[" in line and "]" in line:
                            # Extract technology info
                            tech_matches = re.findall(r'\[([^\]]+)\]', line)
                            for tech in tech_matches:
                                if ":" in tech:
                                    tech_name = tech.split(":")[0].strip()
                                    technologies.add(tech_name)
            
            # Try manual discovery as well
            try:
                # Make request to the URL
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                }
                response = requests.get(url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    # Check headers for technology clues
                    for header, value in response.headers.items():
                        if header.lower() in ["server", "x-powered-by", "x-generator"]:
                            technologies.add(f"{header}: {value}")
                    
                    # Parse HTML for technology clues
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Check meta tags
                    meta_tags = soup.find_all('meta')
                    for tag in meta_tags:
                        if tag.get('name') == 'generator':
                            technologies.add(f"Generator: {tag.get('content', '')}")
                    
                    # Check scripts
                    scripts = soup.find_all('script')
                    for script in scripts:
                        src = script.get('src', '')
                        if src:
                            # Check for common libraries
                            if 'jquery' in src.lower():
                                technologies.add("jQuery")
                            elif 'react' in src.lower():
                                technologies.add("React")
                            elif 'angular' in src.lower():
                                technologies.add("Angular")
                            elif 'vue' in src.lower():
                                technologies.add("Vue.js")
                    
                    # Check for WordPress
                    if soup.find('link', {'rel': 'https://api.w.org/'}):
                        technologies.add("WordPress")
                    
                    # Check for Drupal
                    if soup.find('meta', {'name': 'Generator', 'content': lambda x: x and 'Drupal' in x}):
                        technologies.add("Drupal")
                    
                    # Check CSS for framework clues
                    css_links = soup.find_all('link', {'rel': 'stylesheet'})
                    for css in css_links:
                        href = css.get('href', '')
                        if 'bootstrap' in href.lower():
                            technologies.add("Bootstrap")
                        elif 'foundation' in href.lower():
                            technologies.add("Foundation")
            
            except Exception as e:
                print(f"Error during manual discovery: {e}")
            
            # Create result object
            scan_result = {
                "scan_info": {
                    "target": url,
                    "tool": "technology_discovery",
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                },
                "results": {
                    "technologies": list(technologies),
                    "count": len(technologies)
                },
                "raw_commands": commands_output
            }
            
            # Save scan result
            scan_id = f"tech_{url.replace('://', '_').replace('.', '_').replace('/', '_')}_{int(time.time())}"
            self.scan_results[scan_id] = scan_result
            
            # Save to file
            self._save_scan_result(scan_id, scan_result)
            
            return scan_result
            
        except Exception as e:
            return {
                "error": f"Error discovering technologies: {str(e)}",
                "scan_info": {
                    "target": url,
                    "tool": "technology_discovery"
                }
            }
    
    def get_scan_result(self, scan_id: str) -> Optional[Dict[str, Any]]:
        """
        Get a specific scan result by ID.
        
        Args:
            scan_id: ID of the scan to retrieve
            
        Returns:
            Scan result dictionary or None if not found
        """
        return self.scan_results.get(scan_id)
    
    def get_all_scan_results(self) -> Dict[str, Dict[str, Any]]:
        """
        Get all scan results.
        
        Returns:
            Dictionary of all scan results
        """
        return self.scan_results
    
    def _save_scan_result(self, scan_id: str, result: Dict[str, Any]) -> None:
        """
        Save scan result to a file.
        
        Args:
            scan_id: ID of the scan
            result: Scan result dictionary
        """
        try:
            # Create scan results directory if it doesn't exist
            os.makedirs(SCAN_RESULTS_DIR, exist_ok=True)
            
            # Save scan result to a JSON file
            file_path = os.path.join(SCAN_RESULTS_DIR, f"{scan_id}.json")
            with open(file_path, 'w') as f:
                json.dump(result, f, indent=2)
        except Exception as e:
            print(f"Error saving scan result: {e}")
    
    def load_scan_results(self) -> None:
        """Load all saved scan results from the scan results directory."""
        try:
            # Check if scan results directory exists
            if not os.path.exists(SCAN_RESULTS_DIR):
                return
            
            # Load all JSON files in the scan results directory
            for filename in os.listdir(SCAN_RESULTS_DIR):
                if filename.endswith(".json") and any(prefix in filename for prefix in ["harvester_", "shodan_", "subdomains_", "whois_", "tech_"]):
                    file_path = os.path.join(SCAN_RESULTS_DIR, filename)
                    try:
                        with open(file_path, 'r') as f:
                            result = json.load(f)
                            scan_id = filename[:-5]  # Remove .json extension
                            self.scan_results[scan_id] = result
                    except Exception as e:
                        print(f"Error loading scan result {filename}: {e}")
        except Exception as e:
            print(f"Error loading scan results: {e}")
