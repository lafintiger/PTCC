import os
import json
import time
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
import logging
import asyncio

# Document processing libraries
import pytesseract
from PIL import Image
import fitz  # PyMuPDF for PDF processing
import re
import numpy as np

# Add parent directory to path so we can import from config
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class DocumentAnalyzer:
    """Handles document uploads, analysis, and management."""
    
    def __init__(self, data_dir: str, ollama_client):
        """Initialize the document analyzer."""
        self.data_dir = data_dir
        self.documents_dir = os.path.join(data_dir, "documents")
        self.ollama_client = ollama_client
        self.documents = {}
        self.logger = logging.getLogger("document_analyzer")
        
        # Create documents directory if it doesn't exist
        os.makedirs(self.documents_dir, exist_ok=True)
        
        # Load existing documents
        self.load_documents()
    
    def load_documents(self):
        """Load existing document metadata."""
        metadata_file = os.path.join(self.documents_dir, "metadata.json")
        if os.path.exists(metadata_file):
            try:
                with open(metadata_file, 'r') as f:
                    self.documents = json.load(f)
            except Exception as e:
                self.logger.error(f"Error loading document metadata: {e}")
    
    def save_metadata(self):
        """Save document metadata to file."""
        metadata_file = os.path.join(self.documents_dir, "metadata.json")
        try:
            with open(metadata_file, 'w') as f:
                json.dump(self.documents, f, indent=2)
        except Exception as e:
            self.logger.error(f"Error saving document metadata: {e}")
    
    async def process_document(self, file_path: str, document_name: str, document_type: str, 
                            description: str = "", tags: List[str] = None) -> Dict[str, Any]:
        """
        Process a new document.
        
        Args:
            file_path: Path to the uploaded document
            document_name: Name of the document
            document_type: Type of document (network_diagram, credentials, etc.)
            description: Description of the document
            tags: List of tags for categorizing the document
        
        Returns:
            Document metadata
        """
        if not os.path.exists(file_path):
            return {"error": "File not found"}
        
        # Generate unique ID
        doc_id = str(uuid.uuid4())
        
        # Determine file extension
        _, file_extension = os.path.splitext(file_path)
        file_extension = file_extension.lower()
        
        # Determine file format
        if file_extension in ['.jpg', '.jpeg', '.png']:
            file_format = "image"
        elif file_extension == '.pdf':
            file_format = "pdf"
        else:
            return {"error": "Unsupported file format"}
        
        # Create storage path
        storage_filename = f"{doc_id}{file_extension}"
        storage_path = os.path.join(self.documents_dir, storage_filename)
        
        # Copy file to storage location
        try:
            with open(file_path, 'rb') as src, open(storage_path, 'wb') as dst:
                dst.write(src.read())
        except Exception as e:
            return {"error": f"Error saving file: {str(e)}"}
        
        # Extract text content based on file format
        text_content = ""
        if file_format == "image":
            text_content = self._extract_text_from_image(storage_path)
        elif file_format == "pdf":
            text_content = self._extract_text_from_pdf(storage_path)
        
        # Process with AI
        ai_analysis = await self._analyze_with_ai(text_content, file_format, document_type)
        
        # Create document metadata
        document_meta = {
            "id": doc_id,
            "name": document_name,
            "type": document_type,
            "format": file_format,
            "file_path": storage_path,
            "description": description,
            "tags": tags or [],
            "upload_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "text_content": text_content,
            "ai_analysis": ai_analysis
        }
        
        # Save metadata
        self.documents[doc_id] = document_meta
        self.save_metadata()
        
        return document_meta
    
    def _extract_text_from_image(self, image_path: str) -> str:
        """Extract text from an image using OCR."""
        try:
            img = Image.open(image_path)
            text = pytesseract.image_to_string(img)
            return text
        except Exception as e:
            self.logger.error(f"Error extracting text from image: {e}")
            return ""
    
    def _extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from a PDF document."""
        try:
            text = ""
            with fitz.open(pdf_path) as pdf:
                for page in pdf:
                    text += page.get_text()
            return text
        except Exception as e:
            self.logger.error(f"Error extracting text from PDF: {e}")
            return ""
    
    async def _analyze_with_ai(self, text_content: str, file_format: str, document_type: str) -> Dict[str, Any]:
        """Analyze document content with AI."""
        if not text_content:
            return {"summary": "No text content could be extracted for analysis."}
        
        prompt = self._create_analysis_prompt(text_content, file_format, document_type)
        
        try:
            response = await self.ollama_client.generate(prompt)
            
            # Parse the response
            analysis = {
                "summary": "",
                "entities": [],
                "technical_details": {},
                "security_insights": []
            }
            
            # Simple parsing logic - this would be more sophisticated in a real implementation
            sections = response.split("##")
            for section in sections:
                if "Summary" in section:
                    analysis["summary"] = section.replace("Summary", "").strip()
                elif "Entities" in section:
                    entities_text = section.replace("Entities", "").strip()
                    entities = re.findall(r'- (.*?)(?:\n|$)', entities_text)
                    analysis["entities"] = entities
                elif "Technical Details" in section:
                    tech_text = section.replace("Technical Details", "").strip()
                    # Extract key-value pairs
                    details = {}
                    for line in tech_text.split("\n"):
                        if ":" in line:
                            key, value = line.split(":", 1)
                            details[key.strip()] = value.strip()
                    analysis["technical_details"] = details
                elif "Security Insights" in section:
                    insights_text = section.replace("Security Insights", "").strip()
                    insights = re.findall(r'- (.*?)(?:\n|$)', insights_text)
                    analysis["security_insights"] = insights
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Error analyzing document with AI: {e}")
            return {"error": f"Error during AI analysis: {str(e)}"}
    
    def _create_analysis_prompt(self, text_content: str, file_format: str, document_type: str) -> str:
        """Create a prompt for AI analysis based on document type and content."""
        # Truncate text if too long
        max_text_length = 3000  # Adjust based on model limitations
        if len(text_content) > max_text_length:
            text_content = text_content[:max_text_length] + "..."
        
        if document_type == "network_diagram":
            prompt = f"""Analyze this network diagram text extracted via OCR. The text comes from a {file_format} file.

Text content:
{text_content}

Please provide:
## Summary
A brief summary of what this network diagram shows.

## Entities
List of network entities identified (servers, clients, routers, etc.)

## Technical Details
Network ranges, IP addresses, hostnames, domains, and other technical information identified.

## Security Insights
Potential security issues, attack vectors, or vulnerabilities apparent from the diagram.
"""
        elif document_type == "credentials":
            prompt = f"""Analyze this document containing credential information. The text comes from a {file_format} file.

Text content:
{text_content}

Please provide:
## Summary
A brief summary of credential information found.

## Entities
List of users, accounts, or systems mentioned.

## Technical Details
Types of credentials, formats, or systems they're associated with.

## Security Insights
Security implications of the found credentials, storage concerns, or potential risks.
"""
        else:
            prompt = f"""Analyze this document related to cybersecurity or IT infrastructure. The text comes from a {file_format} file.

Text content:
{text_content}

Please provide:
## Summary
A brief summary of the document contents.

## Entities
List of key entities mentioned (people, systems, organizations).

## Technical Details
Technical information found in the document that might be relevant for security testing.

## Security Insights
Any security implications, potential vulnerabilities, or useful penetration testing insights from this document.
"""
        
        return prompt
    
    def get_document(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """Get document metadata by ID."""
        return self.documents.get(doc_id)
    
    def get_all_documents(self) -> Dict[str, Dict[str, Any]]:
        """Get all documents."""
        return self.documents
    
    def delete_document(self, doc_id: str) -> bool:
        """Delete a document."""
        if doc_id not in self.documents:
            return False
        
        # Get file path
        file_path = self.documents[doc_id].get("file_path")
        
        # Delete file if it exists
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except Exception as e:
                self.logger.error(f"Error deleting file {file_path}: {e}")
        
        # Remove from metadata
        del self.documents[doc_id]
        self.save_metadata()
        
        return True
    
    def search_documents(self, query: str, doc_type: Optional[str] = None, 
                      tags: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        Search documents based on query, type, and tags.
        
        Args:
            query: Text to search for in name, description, and content
            doc_type: Filter by document type
            tags: Filter by tags
            
        Returns:
            List of matching documents
        """
        results = []
        
        for doc_id, doc in self.documents.items():
            # Filter by document type if specified
            if doc_type and doc.get("type") != doc_type:
                continue
            
            # Filter by tags if specified
            if tags and not all(tag in doc.get("tags", []) for tag in tags):
                continue
            
            # Search in text fields
            query_lower = query.lower()
            searchable_text = (
                doc.get("name", "").lower() + " " +
                doc.get("description", "").lower() + " " +
                doc.get("text_content", "").lower()
            )
            
            if query_lower in searchable_text:
                # Create a copy of the doc but exclude large text content
                result = doc.copy()
                if "text_content" in result:
                    # Include only a snippet of text_content
                    full_text = result["text_content"]
                    snippet = full_text[:200] + "..." if len(full_text) > 200 else full_text
                    result["text_snippet"] = snippet
                    del result["text_content"]
                
                results.append(result)
        
        return results
    
    async def update_document_analysis(self, doc_id: str) -> Dict[str, Any]:
        """Update the AI analysis for a document."""
        if doc_id not in self.documents:
            return {"error": "Document not found"}
        
        doc = self.documents[doc_id]
        
        # Re-analyze with AI
        text_content = doc.get("text_content", "")
        file_format = doc.get("format", "")
        document_type = doc.get("type", "")
        
        ai_analysis = await self._analyze_with_ai(text_content, file_format, document_type)
        
        # Update metadata
        doc["ai_analysis"] = ai_analysis
        self.documents[doc_id] = doc
        self.save_metadata()
        
        return doc 